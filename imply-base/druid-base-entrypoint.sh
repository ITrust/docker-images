#!/bin/bash

druid_cfg_file="$IMPLY_HOME/conf/druid/_common/common.runtime.properties"

#######################
# Druid configuration #
#######################
export MASTER_HOST=${MASTER_HOST:=$(grep $HOSTNAME /etc/hosts | awk '{print $1}')}
export MASTER_PORT=${MASTER_PORT:=1527}

#
# host
#

export DRUID_host=${DRUID_host:=$MASTER_HOST}

#
# Extensions
#

export DRUID_extensions_directory=${DRUID_extensions_directory:=dist/druid/extensions}
export DRUID_extensions_hadoopDependenciesDir=${DRUID_extensions_hadoopDependenciesDir:=dist/druid/hadoop-dependencies}
export DRUID_extensions_loadList=${DRUID_extensions_loadList:='["druid-histogram", "druid-datasketches", "druid-kafka-indexing-service"]'}

#
# Logging
#

# Log all runtime properties on startup. Disable to avoid logging properties on startup:
export DRUID_startup_logging_logProperties=${DRUID_startup_logging_logProperties:=true}

#
# Zookeeper
#

export DRUID_zk_service_host=${DRUID_zk_service_host:=zookeeper}
export DRUID_zk_paths_base=${DRUID_zk_paths_base:=/druid}

#
# Metadata storage
#

# For Derby server on your Druid Coordinator (only viable in a cluster with a single Coordinator, no fail-over):
export DRUID_metadata_storage_type=${DRUID_metadata_storage_type:=derby}
export DRUID_metadata_storage_connector_connectURI=${DRUID_metadata_storage_connector_connectURI:=jdbc:derby://$MASTER_HOST:$MASTER_PORT/var/druid/metadata.db;create=true}
export DRUID_metadata_storage_connector_host=${DRUID_metadata_storage_connector_host:=$MASTER_HOST}
export DRUID_metadata_storage_connector_port=${DRUID_metadata_storage_connector_port:=$MASTER_PORT}

#
# Deep storage
#

# For local disk (only viable in a cluster if this is a network mount):
export DRUID_storage_type=${DRUID_storage_type:=local}
export DRUID_storage_storageDirectory=${DRUID_storage_storageDirectory:=var/druid/segments}


#
# Indexing service logs
#

# For local disk (only viable in a cluster if this is a network mount):
export DRUID_indexer_logs_type=${DRUID_indexer_logs_type:=file}
export DRUID_indexer_logs_directory=${DRUID_indexer_logs_directory:=var/druid/indexing-logs}

#
# Service discovery
#

export DRUID_selectors_indexing_serviceName=${DRUID_selectors_indexing_serviceName:=druid/overlord}
export DRUID_selectors_coordinator_serviceName=${DRUID_selectors_coordinator_serviceName:=druid/coordinator}

#
# Monitoring
#

export DRUID_monitoring_monitors=${DRUID_monitoring_monitors:='["com.metamx.metrics.JvmMonitor"]'}
export DRUID_emitter=${DRUID_emitter:=noop}
export DRUID_emitter_logging_logLevel=${DRUID_emitter_logging_logLevel:=info}

echo '# Generated by druid-base-entrypoint.sh' > ${druid_cfg_file}
while read -r var ; do
  key=$(echo $var | sed -r 's/DRUID_(.*)=.*/druid_\1/g' | tr _ .)
  value=$(echo $var | sed -r 's/.*=(.*)/\1/g')
  echo "${key}=${value}" >> ${druid_cfg_file}
done < <(env | grep '^DRUID_' | sort)

echo ""
echo "Druid common runtime configuration:"
echo "-----------------------------------"
cat ${druid_cfg_file}
echo ""

########################
# hadoop configuration #
########################
hadoop_cfg_path="$IMPLY_HOME/conf/druid/_common"

export CORE_CONF_fs_defaultFS=${CORE_CONF_fs_defaultFS:-hdfs://hdfsname:8020}

function addProperty() {
  local path=$1
  local name=$2
  local value=$3

  local entry="<property><name>$name</name><value>${value}</value></property>"
  local escapedEntry=$(echo $entry | sed 's/\//\\\//g')
  sed -i "/<\/configuration>/ s/.*/${escapedEntry}\n&/" $path
}

function configure() {
    local path=$1
    local module=$2
    local envPrefix=$3

    local var
    local value

    echo "Configuring $module"
    for c in `printenv | perl -sne 'print "$1 " if m/^${envPrefix}_(.+?)=.*/' -- -envPrefix=$envPrefix`; do
        name=`echo ${c} | perl -pe 's/___/-/g; s/__/_/g; s/_/./g'`
        var="${envPrefix}_${c}"
        value=${!var}
        echo " - Setting $name=$value"
        addProperty $path/$module-site.xml $name "$value"
    done
}

configure $hadoop_cfg_path core CORE_CONF
configure $hadoop_cfg_path hdfs HDFS_CONF
configure $hadoop_cfg_path yarn YARN_CONF
configure $hadoop_cfg_path httpfs HTTPFS_CONF
configure $hadoop_cfg_path kms KMS_CONF

exec "$@"